# [Speed up]Neural Sparse Voxel Fields

### Motivation：

在经典的计算机图形学技术中，对于真实世界场景进行自由视角渲染，需要**捕获详细外观和几何模型**。而最近尝试在**没有3D监督**的情况下情况下进行情景表示，则由于其**网络容量有限**或**难以找到相机和场景几何体的准确焦点**而导致**模糊渲染**(Blurry renderings), 为获取高分辨率渲染图像则需要耗时的光线行进(Ray marching)。

NSVF定义了一组以**八叉树形式**组织的隐式场，并通过**可微分的光线行进**操作来加速渲染。

在**多场景学习**、**运动物体的自由渲染**以及**大规模场景渲染**中有了较大突破。

### Introduction：

计算机图形学中的逼真渲染具有广泛的应用，包括混合现实、视觉效果、可视化，甚至计算机视觉和机器人导航中的训练数据生成。从任意角度逼真地渲染真实世界场景是一项巨大的挑战，因为通常无法像在高预算的视觉效果制作中那样获得高质量的场景几何图形和材料模型。

因此，研究人员开发了**基于图像的渲染 (IBR) 方法，将基于视觉的场景几何建模与基于图像的视图插值相结合。**

基于建立3D模型的渲染技术不同，基于图象的渲染（Image-based rendering，既IBR）是指仅仅根据在3D场景中按一定的算法，有序拍摄的一组2D图像，计算机使用已经在不同位置拍摄的原始照片，渲染出在一个指定位置的视点的2D图像。而传统的计算机图形技术是先建立一个场景的3D几何模型，然后再根据已经建立起来的场景3D模型，指定相机在3D场景中的精确位置，用计算机投影绘制（渲染）2D视图。

但是其**生成效果欠佳**、并且**基于特定场景**，泛化性较差。

最近的工作采用**神经网络**来隐式地学习**隐式场景**表示，但是仍然由于**网络容量有限**或**难以找到相机和场景几何体的准确焦点**而导致**模糊渲染**，而其中的光线行进环节同样非常耗时，除此之外，通过神经网络来**编辑**或者**合成**3D场景模型仍有很大的研究空间。

对此，NSVF主要有两个大的步骤改进：

* 在体素的每个顶点（共8个）分配一个**特征嵌入**(Embedding)，并且在该体素内的节点通过这八个顶点的特征和三次线性样条插值来获得该点的特征，大大减少了渲染时的计算量（由渲染一个点(x, y, z)需要跑一遍神经网络的前向传播 变成 只需要做一下八个点的插值了）。
* 引入了渐进式的训练策略，通过**可微分**的光线操作，剔除不包含场景信息的稀疏体素，减少了推理时的渲染速度（因为构建八叉树的图中 建立了一个额外的显示数据结构，这个数据结构中提出了不包含场景信息的稀疏体素）。

### Background

**Neural Rendering with Implicit Fields**
$$
F_{\theta}: (p,v) \rightarrow (c,w)
$$
其中$\theta$是神经网络的参数、$c$ 为场景颜色、$p$ 为空间位置、$w$ 为光线 $v$ 处的概率密度。
$$
p_{0} \in R^3
$$
其中针孔相机拍摄，将$H$ $\times$ $W$ 的图像渲染成三维数据。
$$
p(z) = p_{0} + z
$$

$$
C(p_{0},v) = \int_{0}^{+\infty}w(p(z))\cdot c(p(z),v)dz,\ \ where \int_{0}^{+\infty}w(p(z))dz =1
$$

其中为了多视角一致，$w$被限制为$p(z)$的函数。

而在上面公式中，渲染过程中主要有两种计算方式：

**表面渲染** 这种方法假设$w(p(z))$ 为狄拉克$δ$函数，$δ(p(z) - p(z^*))$，其中$p(z^*)$是光线焦点和场景几何的焦点，即：
$$
C(p_{0},v) = \int_{0}^{+\infty}δ(p(z) - p(z^*))\cdot c(p(z),v)dz,\ \ where \int_{0}^{+\infty}w(p(z))dz =1
$$
**体渲染**  体渲染通过密集采样，并对采样点的颜色和密度积分到2D图像中，来估计$C(p_{0},v)$, 这展开上上式为离散形式：
$$
C(p_{0},v) \approx \sum_{i=1}^{N}(\prod_{j=1}^{i-1}\alpha(z_{j}, \triangle_j))\cdot(1-\alpha(z_{i}, \triangle_{i})) \cdot c(p(z_i),v) \\
\alpha(z_i, \triangle_i) = exp(- \delta(p(z_i) \cdot \triangle_i)) \\
$$
其中$\triangle_i = (z_{i+1} - z_i) \cdot \left\{{c(p(z_i),v)}_{i=1}^N \right\}$ 和 $\left\{{\sigma(p(z_i))}_{i=1}^N \right\}$ 是采样点的颜色和密度。

**现有方法的局限性：**

对于表面渲染，找到一个**准确的表面**使得颜色多视角一致，这对训练收敛来说是很困难的。

而对于体渲染，**沿光线采样大量点以进行颜色累积，以实现高质量渲染**。

### Neural Sparse Voxel Fields

$$
v = \left\{ v_1, v_2, ..., v_k\right\}\\
F_{\theta}(p,v) = F_{\theta}^{i}(g_i(p),v)\\
F_{\theta}^{i}(g_i(p),v) \rightarrow (c,\sigma), \forall p \in V_i
$$

其中$v$被表示为一组稀疏体素，$F_{\theta}$ 被表示为神经网络参数，$\sigma$为密度，其他参数如上。

其中$\tilde g(p_1^*)$ 表示为$V_i$的八个顶点，$\chi$ 表示三线性插值，$\zeta$表示后处理函数，其中三个八个体素嵌入信息可以包括区域特定信息（譬如 几何、材料、颜色等）：
$$
g_i (p) = \zeta(\chi(\tilde g(p_1^*), ..., \tilde g(p_8^*)))
$$
**体渲染**

体渲染过程主要分为两步进行：1. 光线-体素相交 2. 在体素内的光线行进。

**Ray-voxel Intersection** 通过AABB测试来判断每条光线是否与体素相交。

![image-20231025143721021](C:\Users\admin\AppData\Roaming\Typora\typora-user-images\image-20231025143721021.png)

**Ray Marching inside Voxels**
$$
C(p_{0},v) \approx \sum_{i=1}^{N}(\prod_{j=1}^{i-1}\alpha(z_{j}, \triangle_j))\cdot(1-\alpha(z_{i}, \triangle_{i})) \cdot c(p(z_i),v) + A(p_0,v) \cdot c_{bg}\\
A(p_0, v) =\prod_{i=1}^N \alpha(z_i, \triangle_i)
$$
与以往渲染公式不同的是，为了处理光线错过所有对象的情况，添加了一个背景项内容。

![image-20231025144332385](C:\Users\admin\AppData\Roaming\Typora\typora-user-images\image-20231025144332385.png)

和之前的Nerf两阶段采样方法不同，通过将与体素交点作为样本，使用其中点进行颜色累积，注意在采样过程中使用了一种偏置采样策略，只对命中至少一个体素的光线进行采样。

**早停机制**
$$
A(p_0, v) > 0.01
$$
**损失函数**
$$
L = \sum_{(p_0,v) \in R}||C(p_0,v) - C^*(p_0,v)||_2^2 +\lambda \cdot \Omega(A(p_0,v))
$$
其中$R$ 是一批采样光线，**这里面的** $\Omega$ 需要查一下，不知道具体实现方法。

**像素初始化**
$$
L = \sqrt[3]{V/1000}
$$
**自我剪枝**
$$
V_i\ \ is\ pruned\ if\ min_{j=1...G}\ exp(-\sigma(g_i(p_j))) > \gamma, p_j \in V_i, V_i \in V
$$
这其中$p_j$是从该体素中随机选取的G个取样点，而$\sigma(g_i(p_j))$ 是对于该采样点的预测密度，$\gamma$被设置为0.5。

**逐步学习**

每次会将体素大小减半，当体素大小减半时，会通过原始八个体素顶点处的特征进行三次样条插值来初始化新顶点的特征表示。

![image-20231025150212335](C:\Users\admin\AppData\Roaming\Typora\typora-user-images\image-20231025150212335.png)

![image-20231025150745312](C:\Users\admin\AppData\Roaming\Typora\typora-user-images\image-20231025150745312.png)

### Ablation Study

**体素嵌入比使用位置编码带来了更大的质量改进**。

**通过更多轮渐进式训练，性能得到改善。 但是经过一定的轮数后，随着渲染时间的增加，质量只会缓慢提高**。

### 限制和未来的工作

尽管 NSVF 可以有效地生成高质量的新视角并显着优于现有方法，但存在三个主要限制：

(i) 我们的方法**无法处理背景复杂的场景**。我们假设一个简单的常数背景项 。但是，从不同的角度观看真实场景时，通常会有不同的背景。这使得在不干扰目标场景学习的情况下正确捕捉它们的效果变得具有挑战性。

(ii) 我们在所有实验中将**自我修剪的阈值设置为 0.5**。虽然这对一般场景很有效，但如果阈值设置不正确，对于非常薄的结构可能会发生不正确的修剪。

(iii) NSVF 学习颜色和密度作为查询点位置和相机光线方向的“黑箱”函数。因此，渲染性能高度**依赖于训练图像的分布**，当训练数据不足以预测复杂的几何、材料和光照效果时，可能会产生严重的伪影。未来一个可能的方向是将传统的辐射和渲染方程作为**物理归纳偏置**纳入神经渲染框架。这可以潜在地提高神经网络模型的鲁棒性和泛化能力。

(iv) 当前的学习范式需要已知的相机姿势作为输入来初始化光线及其方向。 对于真实世界的图像，目前没有处理相机校准中不可避免的错误的机制。 当我们的目标数据由多个对象的单视图图像组成时，在实际应用中获得准确配准的姿态就更加困难。 未来研究的一个有前途的途径是**使用无监督技术，如 GAN同时预测相机姿势**以获得高质量的自由视角渲染结果。

