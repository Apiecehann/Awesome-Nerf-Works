# Instant Neural Graphics Primitives with a Multiresolution Hash Encoding

### Introduction

神经网络训练和推断的成本是很高昂的，多尺度的哈希表能够有效帮助神经网络减少参数量，并提高渲染质量。

其中将神经网络输入经编码**升维**到高纬空间是关键，其中绝大多数的编码方式是**可训练的**。

但是部分数据结构由于其本身维护过程（如修剪、拆分或合并），这会使训练过程复杂化。

提出的**Multiresolution hash encoding**方法仅有两个参数设置：参数量$T$和最佳分辨率$N$

**输入编码升维的几种方式**

**频率编码**

原始Nerf使用的三角函数方法，使用线性变换，将低频转换为高频函数。

**参数编码**

通过安排更大的内存占用更小的计算成本，譬如辅助类型的数据结构：网格或者树。

**稀疏参数编码**

网格中的训练参数要比神经网络的权重使用更多的内存。

但是由于空间的空白部分也会进行分配，但是无用、而且参数的级别是$O(n^3)$，而物体表面的级别是$O(n^2)$，$n$可以理解为分辨率，密集型网格容易过度平滑。

### 多层次哈希编码

![image-20231025201528379](C:\Users\admin\AppData\Roaming\Typora\typora-user-images\image-20231025201528379.png)

方法大致如下：首先将坐标点(x,y,z的真实值)转换为哈希表中的坐标，图中粉色和蓝色的分辨率不同，之后在不同等级的哈希表中找到该点周围所在等级的附近八个点位的值，并进行三次样条插值，将所有的分辨率哈希表和辅助输入内容（包括几何、材质等信息）拼接，完成encoding, 并输入神经网络之中。

![image-20231025202301683](C:\Users\admin\AppData\Roaming\Typora\typora-user-images\image-20231025202301683.png)

上图为哈希表的相关内容设置，而不同的分辨率取值为：
$$
N_l = [N_{min} \cdot b^l] \\
b = exp(\frac{ln N_{max} - ln N_{min}}{L-1})
$$
其中$N_l$ 代表第$l$层的分辨率。

值得注意的是在$N_l$较小时，网格的数量是小于哈希表的容量，所以不需要Hash映射。

但是在$N_l$ 较大时，网格的数量是大于哈希表容量的，所以需要Hash映射问题。

在训练过程中完全不考虑额外的索引冲突问题。

在$N_l$较大时，Hash映射如下：
$$
h(x) = ( \bigoplus_{i=1}^{d} x_i \pi_i) \ mod \ T
$$
其中$\bigoplus$ 代表每维度取余，而其中三维的$\pi_1 = 1$, $\pi_2 = 2654435761$, $\pi_3 = 805459861$

### 额外的一些Tips

* 关于T的取值很可能是越高越好，因为GPU3090的限制，所以取值到了 $2^{19}$
* 一些比较好的提升方法是解决在高$N_l$部分的Hash冲突。